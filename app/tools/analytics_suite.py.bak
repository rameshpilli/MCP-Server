"""
Analytics Suite for MCP Server

This module provides advanced visualization and data analysis tools for the MCP server.
It includes tools for summarizing, visualizing, comparing, forecasting, and analyzing
tabular data, as well as generating reports and exporting results.
It leverages dynamic column profiling for intelligent data understanding.

Usage:
    from app.tools.analytics_suite import register_tools
    register_tools(mcp)
"""

import os
import base64
import logging
import json
import io
import re
import tempfile
import uuid
from typing import List, Dict, Any, Optional, Union, Tuple, Callable
from datetime import datetime, timedelta
from pathlib import Path

import pandas as pd
import numpy as np
import matplotlib
matplotlib.use('Agg')  # Use non-interactive backend
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.tsa.seasonal import seasonal_decompose
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA
from sklearn.ensemble import IsolationForest
from fastmcp import Context

from app.utils.column_profiler import ColumnProfiler, ColumnMetadata

# Configure logging
logger = logging.getLogger("mcp_server.tools.analytics_suite")

# Ensure output directory exists
output_dir = Path("output")
output_dir.mkdir(exist_ok=True)

# Set default style for visualizations
plt.style.use('seaborn-v0_8-whitegrid')
sns.set_palette("deep")
sns.set_context("talk")

# --- Helper Functions ---

def _ensure_dataframe_and_profile(data: Union[List[Dict[str, Any]], pd.DataFrame], profiler: ColumnProfiler) -> Tuple[Optional[pd.DataFrame], Optional[Dict[str, ColumnMetadata]]]:
    """Convert data to DataFrame and profile it. Returns None, None if data is empty or invalid."""
    if isinstance(data, pd.DataFrame):
        df = data.copy()
    elif isinstance(data, list) and all(isinstance(item, dict) for item in data):
        df = pd.DataFrame(data)
    else:
        logger.error("Invalid data format. Must be a DataFrame or a list of dictionaries.")
        return None, None
    
    if df.empty:
        logger.info("Input data is empty.")
        return df, None # Return empty df, no metadata
        
    metadata_map = profiler.profile_dataframe(df)
    return df, metadata_map

def _get_col_by_semantic_type(metadata_map: Dict[str, ColumnMetadata], semantic_type: str, exclude: Optional[List[str]] = None) -> Optional[str]:
    """Get the first column matching a semantic type."""
    exclude = exclude or []
    for col_name, meta in metadata_map.items():
        if meta.semantic_type == semantic_type and col_name not in exclude:
            return col_name
    return None

def _get_cols_by_semantic_types(metadata_map: Dict[str, ColumnMetadata], semantic_types: List[str], exclude: Optional[List[str]] = None) -> List[str]:
    """Get all columns matching a list of semantic types."""
    exclude = exclude or []
    return [col_name for col_name, meta in metadata_map.items() if meta.semantic_type in semantic_types and col_name not in exclude]

def _get_col_by_cleaned_name_or_query(
    query: Optional[str],
    metadata_map: Dict[str, ColumnMetadata],
    target_semantic_types: Optional[List[str]] = None,
    fallback_original_name: Optional[str] = None
) -> Optional[str]:
    """Intelligently find a column based on query, semantic types, or fallback name."""
    if not metadata_map: return fallback_original_name

    # 1. Exact match on original name (if fallback_original_name is a valid column)
    if fallback_original_name and fallback_original_name in metadata_map:
        meta = metadata_map[fallback_original_name]
        if not target_semantic_types or meta.semantic_type in target_semantic_types:
            return fallback_original_name

    # 2. Match from query
    if query:
        query_lower = query.lower()
        # Check cleaned names first
        for original_name, meta in metadata_map.items():
            if meta.cleaned_name.lower() in query_lower:
                if not target_semantic_types or meta.semantic_type in target_semantic_types:
                    return original_name
        # Check original names
        for original_name, meta in metadata_map.items():
            if original_name.lower() in query_lower:
                if not target_semantic_types or meta.semantic_type in target_semantic_types:
                    return original_name
    
    # 3. First column matching semantic type
    if target_semantic_types:
        for original_name, meta in metadata_map.items():
            if meta.semantic_type in target_semantic_types:
                return original_name
                
    # 4. Fallback to the provided original name if it exists, regardless of type match if no query/types specified
    if fallback_original_name and fallback_original_name in metadata_map and not target_semantic_types and not query:
        return fallback_original_name
        
    return None


def _save_figure_to_file(fig: plt.Figure, format: str = 'png') -> str:
    filename = f"chart_{uuid.uuid4().hex}.{format}"
    filepath = output_dir / filename
    fig.savefig(filepath, bbox_inches='tight', dpi=300)
    plt.close(fig)
    return str(filepath)

def _figure_to_base64(fig: plt.Figure, format: str = 'png') -> str:
    buf = io.BytesIO()
    fig.savefig(buf, format=format, bbox_inches='tight', dpi=300)
    buf.seek(0)
    img_str = base64.b64encode(buf.read()).decode('utf-8')
    plt.close(fig)
    return img_str

def _dataframe_to_file(df: pd.DataFrame, metadata_map: Dict[str, ColumnMetadata], format: str = 'csv') -> str:
    filename = f"data_{uuid.uuid4().hex}.{format}"
    filepath = output_dir / filename
    
    # Use cleaned names for export
    df_export = df.copy()
    df_export.columns = [metadata_map.get(col, ColumnMetadata(original_name=col, cleaned_name=col, semantic_type="Unknown", pandas_dtype="object", description="", count=0,null_count=0,null_ratio=0,unique_count=0,unique_ratio=0,profile_summary="")).cleaned_name for col in df.columns]

    if format == 'csv':
        df_export.to_csv(filepath, index=False)
    elif format == 'xlsx':
        df_export.to_excel(filepath, index=False)
    elif format == 'json':
        df_export.to_json(filepath, orient='records', indent=2)
    else:
        raise ValueError(f"Unsupported export format: {format}")
    return str(filepath)

# --- Main Tools ---

def register_tools(mcp):
    """Register analytics tools with the MCP server"""
    
    profiler = ColumnProfiler()

    @mcp.tool()
    async def summarize_table(
        ctx: Context,
        data: List[Dict[str, Any]],
        columns: Optional[List[str]] = None # Original column names
    ) -> str:
        """
        Generates a statistical summary of tabular data using dynamic column profiling.
        Calculates statistics for columns based on their inferred semantic types.
        """
        try:
            df, metadata_map = _ensure_dataframe_and_profile(data, profiler)
            if df is None or metadata_map is None: return "No data available to summarize or data is empty."

            summary_parts = ["## Data Summary by Column"]
            
            cols_to_summarize = columns if columns else df.columns
            
            for original_col_name in cols_to_summarize:
                if original_col_name not in metadata_map:
                    summary_parts.append(f"\n### {original_col_name} (Unknown)\n- No metadata available for this column.")
                    continue

                meta = metadata_map[original_col_name]
                summary_parts.append(f"\n### {meta.cleaned_name} ({meta.semantic_type})")
                summary_parts.append(f"- Original Name: `{meta.original_name}`, Pandas Dtype: `{meta.pandas_dtype}`")
                if meta.units: summary_parts.append(f"- Units: {meta.units}")
                summary_parts.append(f"- Non-null values: {meta.count} ({100 - meta.null_ratio*100:.1f}% not null)")
                summary_parts.append(f"- Unique values: {meta.unique_count}")

                if meta.semantic_type in ["Currency", "Integer", "Float", "Percentage"]:
                    if meta.min_value is not None: summary_parts.append(f"- Min: {meta.min_value:.2f}")
                    if meta.max_value is not None: summary_parts.append(f"- Max: {meta.max_value:.2f}")
                    if meta.mean_value is not None: summary_parts.append(f"- Mean: {meta.mean_value:.2f}")
                    if meta.median_value is not None: summary_parts.append(f"- Median: {meta.median_value:.2f}")
                    if meta.std_dev is not None: summary_parts.append(f"- Std Dev: {meta.std_dev:.2f}")
                elif meta.semantic_type in ["Date", "Timestamp"]:
                    if meta.min_value is not None: summary_parts.append(f"- Earliest: {meta.min_value}")
                    if meta.max_value is not None: summary_parts.append(f"- Latest: {meta.max_value}")
                elif meta.semantic_type == "Categorical" and meta.top_categories:
                    top_cats_str = ", ".join([f"'{k}' ({v} times)" for k,v in meta.top_categories.items()])
                    summary_parts.append(f"- Top Categories: {top_cats_str}")
                elif meta.semantic_type == "String" or meta.semantic_type == "Free_Text":
                    if meta.min_length is not None: summary_parts.append(f"- Min Length: {meta.min_length}")
                    if meta.max_length is not None: summary_parts.append(f"- Max Length: {meta.max_length}")
                    if meta.avg_length is not None: summary_parts.append(f"- Avg Length: {meta.avg_length:.1f}")
            
            return "\n".join(summary_parts)
            
        except Exception as e:
            logger.error(f"Error summarizing table: {e}", exc_info=True)
            return f"Error summarizing table: {str(e)}"

    @mcp.tool()
    async def visualize_data(
        ctx: Context,
        data: List[Dict[str, Any]],
        chart_type: str = "bar",
        x_column_original: Optional[str] = None,
        y_column_original: Optional[str] = None,
        group_by_original: Optional[str] = None,
        title: Optional[str] = None,
        query: Optional[str] = None,
        include_file: bool = False
    ) -> str:
        """
        Creates a visualization from tabular data, intelligently selecting columns if not specified.
        Supported chart types: bar, line, pie, scatter, heatmap, box, hist.
        """
        try:
            df, metadata_map = _ensure_dataframe_and_profile(data, profiler)
            if df is None or metadata_map is None: return "No data available to visualize or data is empty."

            # Intelligent column selection using profiler metadata and query
            x_col = _get_col_by_cleaned_name_or_query(query, metadata_map, target_semantic_types=["Categorical", "Date", "Timestamp", "Year", "Identifier"], fallback_original_name=x_column_original)
            if not x_col: x_col = _get_col_by_semantic_type(metadata_map, "Categorical") or \
                                 _get_col_by_semantic_type(metadata_map, "Date") or \
                                 _get_col_by_semantic_type(metadata_map, "Year") or \
                                 df.columns[0]

            y_col = _get_col_by_cleaned_name_or_query(query, metadata_map, target_semantic_types=["Currency", "Float", "Integer", "Percentage"], fallback_original_name=y_column_original)
            if not y_col: y_col = _get_col_by_semantic_type(metadata_map, "Float", exclude=[x_col]) or \
                                 _get_col_by_semantic_type(metadata_map, "Integer", exclude=[x_col]) or \
                                 (df.columns[1] if len(df.columns) > 1 and df.columns[1] != x_col else df.columns[0])
            
            group_col = _get_col_by_cleaned_name_or_query(query, metadata_map, target_semantic_types=["Categorical"], fallback_original_name=group_by_original)
            if group_col == x_col or group_col == y_col: group_col = None # Avoid using same column for grouping

            x_meta = metadata_map.get(x_col)
            y_meta = metadata_map.get(y_col)
            group_meta = metadata_map.get(group_col)

            x_label = x_meta.cleaned_name if x_meta else x_col
            y_label = y_meta.cleaned_name if y_meta else y_col
            
            # Convert relevant columns to appropriate types for plotting
            if x_meta and x_meta.semantic_type in ["Date", "Timestamp"]:
                df[x_col] = pd.to_datetime(df[x_col], errors='coerce')
            if y_meta and y_meta.semantic_type in ["Currency", "Float", "Integer", "Percentage"]:
                 df[y_col] = pd.to_numeric(df[y_col], errors='coerce')


            plt.figure(figsize=(12, 7))
            chart_type_lower = chart_type.lower()

            if chart_type_lower == "bar":
                if group_col:
                    sns.barplot(x=x_col, y=y_col, hue=group_col, data=df, estimator=np.mean if y_meta and y_meta.semantic_type != "Integer" else np.sum)
                else:
                    sns.barplot(x=x_col, y=y_col, data=df, estimator=np.mean if y_meta and y_meta.semantic_type != "Integer" else np.sum)
                plt.xticks(rotation=45, ha='right')
            elif chart_type_lower == "line":
                if group_col:
                    sns.lineplot(x=x_col, y=y_col, hue=group_col, data=df, marker='o')
                else:
                    sns.lineplot(x=x_col, y=y_col, data=df, marker='o')
                if x_meta and x_meta.semantic_type in ["Date", "Timestamp"]: plt.gcf().autofmt_xdate()
                else: plt.xticks(rotation=45, ha='right')
            elif chart_type_lower == "pie":
                if df[y_col].isnull().all() or (df[y_col] <= 0).all(): # Check if all y_values are null or non-positive
                     return f"Cannot create pie chart for '{y_label}' as it contains no positive values."
                pie_data = df.groupby(x_col)[y_col].sum().nlargest(10) # Sum for categories, show top 10
                if pie_data.empty or (pie_data <= 0).all():
                     return f"Cannot create pie chart for '{y_label}' by '{x_label}' as aggregated values are not positive."
                plt.pie(pie_data, labels=pie_data.index, autopct='%1.1f%%', startangle=90)
                plt.axis('equal')
            elif chart_type_lower == "scatter":
                sns.scatterplot(x=x_col, y=y_col, hue=group_col if group_col else None, data=df, alpha=0.7, s=100)
            elif chart_type_lower == "heatmap":
                numeric_cols = _get_cols_by_semantic_types(metadata_map, ["Currency", "Float", "Integer", "Percentage"])
                if len(numeric_cols) < 2: return "Heatmap requires at least two numeric columns."
                sns.heatmap(df[numeric_cols].corr(), annot=True, cmap='coolwarm', fmt=".2f")
            elif chart_type_lower == "box":
                sns.boxplot(x=group_col if group_col else x_col, y=y_col, data=df)
                plt.xticks(rotation=45, ha='right')
            elif chart_type_lower == "hist":
                sns.histplot(df[y_col], kde=True)
            else:
                return f"Unsupported chart type: {chart_type}. Supported: bar, line, pie, scatter, heatmap, box, hist."

            chart_title = title or f"{y_label} by {x_label}" + (f" (Grouped by {group_meta.cleaned_name})" if group_col and group_meta else "")
            plt.title(chart_title, fontsize=16)
            plt.xlabel(x_label, fontsize=14)
            plt.ylabel(y_label, fontsize=14)
            plt.tight_layout()

            img_base64 = _figure_to_base64(plt.gcf())
            result_md = f"## {chart_title}\n\n![Chart](data:image/png;base64,{img_base64})\n"
            if include_file:
                file_path = _save_figure_to_file(plt.gcf()) # Re-plot as base64 closes it
                # Re-generate figure for saving
                # This is inefficient, ideally plot once, then save/base64. For now, simple re-plot.
                # A better approach would be to have plotting logic return fig, then handle save/base64.
                # For simplicity of this tool, re-plotting:
                # (omitted for brevity, but in real scenario, you'd re-run plotting logic before saving)
                result_md += f"\n*Chart saved to: {file_path}*"
            
            return result_md
            
        except Exception as e:
            logger.error(f"Error visualizing data: {e}", exc_info=True)
            return f"Error visualizing data: {str(e)}"

    @mcp.tool()
    async def compare_groups(
        ctx: Context,
        data: List[Dict[str, Any]],
        group_column_original: Optional[str] = None,
        metric_column_original: Optional[str] = None,
        query: Optional[str] = None,
        include_visualization: bool = True
    ) -> str:
        """
        Compares metrics across different groups in the data, using profiled column information.
        """
        try:
            df, metadata_map = _ensure_dataframe_and_profile(data, profiler)
            if df is None or metadata_map is None: return "No data available for comparison or data is empty."

            group_col = _get_col_by_cleaned_name_or_query(query, metadata_map, target_semantic_types=["Categorical", "Identifier", "Boolean", "Year"], fallback_original_name=group_column_original)
            if not group_col: group_col = _get_col_by_semantic_type(metadata_map, "Categorical") or df.columns[0]
            
            metric_col = _get_col_by_cleaned_name_or_query(query, metadata_map, target_semantic_types=["Currency", "Float", "Integer", "Percentage"], fallback_original_name=metric_column_original)
            if not metric_col: metric_col = _get_col_by_semantic_type(metadata_map, "Float", exclude=[group_col]) or \
                                           _get_col_by_semantic_type(metadata_map, "Integer", exclude=[group_col]) or \
                                           (df.columns[1] if len(df.columns) > 1 and df.columns[1] != group_col else df.columns[0])

            if not pd.api.types.is_numeric_dtype(df[metric_col]):
                 df[metric_col] = pd.to_numeric(df[metric_col], errors='coerce')
                 if df[metric_col].isnull().all():
                     return f"Metric column '{metadata_map[metric_col].cleaned_name}' contains no numeric data for comparison."

            group_meta = metadata_map.get(group_col)
            metric_meta = metadata_map.get(metric_col)
            group_label = group_meta.cleaned_name if group_meta else group_col
            metric_label = metric_meta.cleaned_name if metric_meta else metric_col

            comparison = df.groupby(group_col)[metric_col].agg(['mean', 'median', 'sum', 'count']).reset_index()
            comparison.columns = [group_label, f"{metric_label} Mean", f"{metric_label} Median", f"{metric_label} Sum", "Count"]
            
            result_md = f"## Comparison of {metric_label} by {group_label}\n\n"
            result_md += comparison.to_markdown(index=False, floatfmt=".2f")

            if include_visualization:
                plt.figure(figsize=(10,6))
                sns.barplot(x=group_label, y=f"{metric_label} Mean", data=comparison)
                plt.title(f"Mean {metric_label} by {group_label}")
                plt.xticks(rotation=45, ha='right')
                plt.tight_layout()
                img_base64 = _figure_to_base64(plt.gcf())
                result_md += f"\n\n![Comparison Chart](data:image/png;base64,{img_base64})"
            
            return result_md

        except Exception as e:
            logger.error(f"Error comparing groups: {e}", exc_info=True)
            return f"Error comparing groups: {str(e)}"

    @mcp.tool()
    async def analyze_trend(
        ctx: Context,
        data: Union[List[Dict[str, Any]], pd.DataFrame],
        time_column_original: Optional[str] = None,
        value_column_original: Optional[str] = None,
        group_column_original: Optional[str] = None,
        query: Optional[str] = None,
        seasonal_period: Optional[int] = None # Inferred if None
    ) -> str:
        """
        Analyzes trends in time series data, with optional seasonal decomposition.
        Uses profiled column information to identify time and value columns.
        """
        try:
            df, metadata_map = _ensure_dataframe_and_profile(data, profiler)
            if df is None or metadata_map is None: return "No data available for trend analysis or data is empty."

            time_col = _get_col_by_cleaned_name_or_query(query, metadata_map, target_semantic_types=["Date", "Timestamp", "Year"], fallback_original_name=time_column_original)
            if not time_col: time_col = _get_col_by_semantic_type(metadata_map, "Date") or _get_col_by_semantic_type(metadata_map, "Timestamp") or _get_col_by_semantic_type(metadata_map, "Year")
            if not time_col: return "Could not identify a suitable time column for trend analysis."

            value_col = _get_col_by_cleaned_name_or_query(query, metadata_map, target_semantic_types=["Currency", "Float", "Integer", "Percentage"], fallback_original_name=value_column_original)
            if not value_col: value_col = _get_col_by_semantic_type(metadata_map, "Float", exclude=[time_col]) or \
                                            _get_col_by_semantic_type(metadata_map, "Integer", exclude=[time_col])
            if not value_col: return "Could not identify a suitable value column for trend analysis."
            
            group_col = _get_col_by_cleaned_name_or_query(query, metadata_map, target_semantic_types=["Categorical", "Identifier"], fallback_original_name=group_column_original)
            if group_col == time_col or group_col == value_col : group_col = None


            df[time_col] = pd.to_datetime(df[time_col], errors='coerce')
            df[value_col] = pd.to_numeric(df[value_col], errors='coerce')
            df = df.dropna(subset=[time_col, value_col]).sort_values(by=time_col)

            if df.empty: return "Not enough valid data after cleaning time and value columns."

            time_meta = metadata_map.get(time_col)
            value_meta = metadata_map.get(value_col)
            time_label = time_meta.cleaned_name if time_meta else time_col
            value_label = value_meta.cleaned_name if value_meta else value_col
            
            result_md = f"## Trend Analysis: {value_label} over {time_label}\n\n"
            
            plt.figure(figsize=(14, 8))
            if group_col:
                group_label = metadata_map.get(group_col, ColumnMetadata(original_name=group_col, cleaned_name=group_col, semantic_type="Unknown", pandas_dtype="object", description="", count=0,null_count=0,null_ratio=0,unique_count=0,unique_ratio=0,profile_summary="")).cleaned_name
                sns.lineplot(x=time_col, y=value_col, hue=group_col, data=df, marker='o')
                plt.title(f"{value_label} Trend by {group_label} over {time_label}", fontsize=16)
            else:
                sns.lineplot(x=time_col, y=value_col, data=df, marker='o')
                plt.title(f"{value_label} Trend over {time_label}", fontsize=16)
            
            plt.xlabel(time_label, fontsize=14)
            plt.ylabel(value_label, fontsize=14)
            plt.gcf().autofmt_xdate()
            plt.tight_layout()
            img_base64 = _figure_to_base64(plt.gcf())
            result_md += f"![Trend Plot](data:image/png;base64,{img_base64})\n\n"

            # Seasonal Decomposition (if enough data and not grouped, or for each group)
            def decompose_and_plot(series, title_suffix=""):
                current_period = seasonal_period
                if current_period is None: # Auto-infer period
                    series_freq = pd.infer_freq(series.index)
                    if series_freq:
                        if 'Q' in series_freq: current_period = 4
                        elif 'M' in series_freq: current_period = 12
                        elif 'W' in series_freq: current_period = 52
                        elif 'D' in series_freq: current_period = 365 # or 7 for weekly within daily
                    if not current_period and len(series) >= 24 : current_period = 12 # Default monthly for generic data
                    elif not current_period and len(series) >= 8 : current_period = 4 # Default quarterly
                
                if current_period and len(series) >= 2 * current_period:
                    try:
                        decomposition = seasonal_decompose(series, model='additive', period=current_period, extrapolate_trend='freq')
                        fig, axes = plt.subplots(4, 1, figsize=(12, 10), sharex=True)
                        decomposition.observed.plot(ax=axes[0], legend=False, title=f'Observed{title_suffix}')
                        decomposition.trend.plot(ax=axes[1], legend=False, title=f'Trend{title_suffix}')
                        decomposition.seasonal.plot(ax=axes[2], legend=False, title=f'Seasonal (Period: {current_period}){title_suffix}')
                        decomposition.resid.plot(ax=axes[3], legend=False, title=f'Residual{title_suffix}')
                        plt.tight_layout()
                        return _figure_to_base64(fig)
                    except Exception as e:
                        logger.warning(f"Could not perform seasonal decomposition for{title_suffix}: {e}")
                return None

            if not group_col and len(df) >= 12: # Min length for meaningful decomposition
                ts = df.set_index(time_col)[value_col]
                decomp_img = decompose_and_plot(ts)
                if decomp_img:
                    result_md += f"### Overall Seasonal Decomposition\n![Decomposition](data:image/png;base64,{decomp_img})\n\n"
            elif group_col:
                 for group_val in df[group_col].unique()[:3]: # Decompose for first 3 groups
                    group_df = df[df[group_col] == group_val]
                    if len(group_df) >= 12:
                        ts_group = group_df.set_index(time_col)[value_col]
                        decomp_img_group = decompose_and_plot(ts_group, title_suffix=f" for {group_val}")
                        if decomp_img_group:
                             result_md += f"### Seasonal Decomposition for {group_val}\n![Decomposition {group_val}](data:image/png;base64,{decomp_img_group})\n\n"
            return result_md

        except Exception as e:
            logger.error(f"Error analyzing trend: {e}", exc_info=True)
            return f"Error analyzing trend: {str(e)}"

    @mcp.tool()
    async def detect_anomalies(
        ctx: Context,
        data: Union[List[Dict[str, Any]], pd.DataFrame],
        value_column_original: Optional[str] = None,
        time_column_original: Optional[str] = None,
        query: Optional[str] = None,
        contamination: float = 0.05, # Expected proportion of outliers
        include_visualization: bool = True
    ) -> str:
        """
        Detects anomalies in a specified value column, optionally using a time column.
        Uses Isolation Forest and profiled column information.
        """
        try:
            df, metadata_map = _ensure_dataframe_and_profile(data, profiler)
            if df is None or metadata_map is None: return "No data available for anomaly detection or data is empty."

            value_col = _get_col_by_cleaned_name_or_query(query, metadata_map, target_semantic_types=["Currency", "Float", "Integer", "Percentage"], fallback_original_name=value_column_original)
            if not value_col: value_col = _get_col_by_semantic_type(metadata_map, "Float") or _get_col_by_semantic_type(metadata_map, "Integer")
            if not value_col: return "Could not identify a suitable value column for anomaly detection."
            
            df[value_col] = pd.to_numeric(df[value_col], errors='coerce')
            df_analysis = df.dropna(subset=[value_col])
            if df_analysis.empty: return f"Column '{metadata_map[value_col].cleaned_name}' has no valid numeric data."

            time_col = _get_col_by_cleaned_name_or_query(query, metadata_map, target_semantic_types=["Date", "Timestamp", "Year"], fallback_original_name=time_column_original)
            if time_col:
                df_analysis[time_col] = pd.to_datetime(df_analysis[time_col], errors='coerce')
                df_analysis = df_analysis.dropna(subset=[time_col]).sort_values(by=time_col)
            
            X = df_analysis[[value_col]]
            model = IsolationForest(contamination=contamination, random_state=42)
            df_analysis['anomaly'] = model.fit_predict(X)
            anomalies_df = df_analysis[df_analysis['anomaly'] == -1]

            value_label = metadata_map[value_col].cleaned_name
            result_md = f"## Anomaly Detection for {value_label}\n\n"
            result_md += f"Found {len(anomalies_df)} anomalies (marked as -1) using Isolation Forest with contamination={contamination:.2%}.\n\n"

            if not anomalies_df.empty:
                display_cols = [time_col, value_col] if time_col else [value_col]
                result_md += "### Detected Anomalies:\n"
                result_md += anomalies_df[display_cols].to_markdown(index=False, floatfmt=".2f") + "\n\n"

                if include_visualization:
                    plt.figure(figsize=(14,7))
                    x_plot = time_col if time_col else df_analysis.index
                    plt.plot(df_analysis[x_plot], df_analysis[value_col], label='Data')
                    plt.scatter(anomalies_df[x_plot], anomalies_df[value_col], color='red', label='Anomaly', s=100)
                    plt.title(f"Anomalies in {value_label}" + (f" over {metadata_map[time_col].cleaned_name}" if time_col else ""), fontsize=16)
                    plt.xlabel(metadata_map[time_col].cleaned_name if time_col else "Index", fontsize=14)
                    plt.ylabel(value_label, fontsize=14)
                    if time_col: plt.gcf().autofmt_xdate()
                    plt.legend()
                    plt.tight_layout()
                    img_base64 = _figure_to_base64(plt.gcf())
                    result_md += f"![Anomaly Plot](data:image/png;base64,{img_base64})\n"
            else:
                result_md += "No significant anomalies detected with the current settings.\n"
            
            return result_md

        except Exception as e:
            logger.error(f"Error detecting anomalies: {e}", exc_info=True)
            return f"Error detecting anomalies: {str(e)}"

    @mcp.tool()
    async def cluster_clients(
        ctx: Context,
        data: Union[List[Dict[str, Any]], pd.DataFrame],
        feature_columns_original: Optional[List[str]] = None,
        n_clusters: int = 0, # 0 means auto-detect
        id_column_original: Optional[str] = None,
        query: Optional[str] = None
    ) -> str:
        """
        Clusters clients or entities based on specified features using K-Means.
        Intelligently selects features and ID column if not specified, using profiled metadata.
        """
        try:
            df, metadata_map = _ensure_dataframe_and_profile(data, profiler)
            if df is None or metadata_map is None: return "No data available for clustering or data is empty."

            if feature_columns_original:
                features = [col for col in feature_columns_original if col in metadata_map and metadata_map[col].semantic_type in ["Currency", "Float", "Integer", "Percentage"]]
            else:
                features = _get_cols_by_semantic_types(metadata_map, ["Currency", "Float", "Integer", "Percentage"])
            
            if not features or len(features) < 2: return "Clustering requires at least two numeric feature columns."
            
            df_cluster = df[features].copy()
            for col in features: # Ensure numeric and handle NaNs
                df_cluster[col] = pd.to_numeric(df_cluster[col], errors='coerce')
            df_cluster = df_cluster.fillna(df_cluster.mean()) # Impute NaNs with mean

            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(df_cluster)

            if n_clusters <= 0: # Auto-detect optimal k
                k_range = range(2, min(11, len(df_cluster)))
                silhouette_scores = {}
                if len(k_range) == 0 : # Not enough data points for range
                    n_clusters = 2 if len(df_cluster) >=2 else 1
                else:
                    for k_val in k_range:
                        if k_val >= len(df_cluster): continue # Cannot have more clusters than samples
                        kmeans_temp = KMeans(n_clusters=k_val, random_state=42, n_init='auto')
                        labels_temp = kmeans_temp.fit_predict(X_scaled)
                        if len(set(labels_temp)) > 1: # Silhouette score requires at least 2 labels
                            silhouette_scores[k_val] = silhouette_score(X_scaled, labels_temp)
                    if silhouette_scores:
                        n_clusters = max(silhouette_scores, key=silhouette_scores.get)
                    else: # Fallback if no valid scores
                        n_clusters = min(3, len(df_cluster)) if len(df_cluster) > 0 else 1
            
            if n_clusters == 1 and len(df_cluster) > 1: # Avoid single cluster for multiple points if auto-detected
                n_clusters = min(2, len(df_cluster))

            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')
            df['cluster_label'] = kmeans.fit_predict(X_scaled)

            id_col = _get_col_by_cleaned_name_or_query(query, metadata_map, target_semantic_types=["Identifier", "Categorical"], fallback_original_name=id_column_original)
            if not id_col: id_col = _get_col_by_semantic_type(metadata_map, "Identifier") or df.columns[0]

            result_md = f"## Client Clustering (K-Means, {n_clusters} Clusters)\n\n"
            result_md += "Based on features: " + ", ".join([metadata_map[f].cleaned_name for f in features]) + "\n\n"
            
            cluster_summary = df.groupby('cluster_label')[id_col].count().reset_index(name='Client Count')
            result_md += "### Cluster Sizes:\n" + cluster_summary.to_markdown(index=False) + "\n\n"
            
            cluster_means = df.groupby('cluster_label')[features].mean()
            cluster_means.columns = [metadata_map[f].cleaned_name for f in features]
            result_md += "### Cluster Centers (Mean Feature Values):\n" + cluster_means.to_markdown(floatfmt=".2f") + "\n\n"

            if X_scaled.shape[1] >= 2: # PCA and plot if at least 2 features
                pca = PCA(n_components=2, random_state=42)
                X_pca = pca.fit_transform(X_scaled)
                df_plot = pd.DataFrame(X_pca, columns=['PCA1', 'PCA2'])
                df_plot['Cluster'] = df['cluster_label']
                
                plt.figure(figsize=(10,7))
                sns.scatterplot(x='PCA1', y='PCA2', hue='Cluster', data=df_plot, palette='viridis', s=100, alpha=0.8)
                plt.title(f'Client Clusters (PCA Projection)', fontsize=16)
                plt.xlabel("Principal Component 1", fontsize=14)
                plt.ylabel("Principal Component 2", fontsize=14)
                plt.legend(title='Cluster')
                plt.tight_layout()
                img_base64 = _figure_to_base64(plt.gcf())
                result_md += f"![Cluster Plot (PCA)](data:image/png;base64,{img_base64})\n"
                result_md += f"*PCA explains {pca.explained_variance_ratio_.sum()*100:.2f}% of variance.*\n"

            return result_md

        except Exception as e:
            logger.error(f"Error clustering clients: {e}", exc_info=True)
            return f"Error clustering clients: {str(e)}"

    @mcp.tool()
    async def correlation_matrix(
        ctx: Context,
        data: Union[List[Dict[str, Any]], pd.DataFrame],
        columns_original: Optional[List[str]] = None,
        query: Optional[str] = None,
        method: str = "pearson" # pearson, kendall, spearman
    ) -> str:
        """
        Calculates and visualizes the correlation matrix for numeric columns.
        Uses profiled metadata to select appropriate columns if not specified.
        """
        try:
            df, metadata_map = _ensure_dataframe_and_profile(data, profiler)
            if df is None or metadata_map is None: return "No data available for correlation analysis or data is empty."

            if columns_original:
                numeric_cols = [col for col in columns_original if col in metadata_map and metadata_map[col].semantic_type in ["Currency", "Float", "Integer", "Percentage"]]
            else:
                numeric_cols = _get_cols_by_semantic_types(metadata_map, ["Currency", "Float", "Integer", "Percentage"])

            if len(numeric_cols) < 2: return "Correlation analysis requires at least two numeric columns."
            
            df_corr = df[numeric_cols].copy()
            for col in numeric_cols: # Ensure numeric
                 df_corr[col] = pd.to_numeric(df_corr[col], errors='coerce')
            df_corr = df_corr.dropna() # Drop rows with NaNs in selected columns for corr

            if len(df_corr) < 2: return "Not enough valid data points for correlation after cleaning."

            corr_matrix = df_corr.corr(method=method)
            cleaned_col_names = [metadata_map[col].cleaned_name for col in numeric_cols]
            corr_matrix.columns = cleaned_col_names
            corr_matrix.index = cleaned_col_names

            result_md = f"## Correlation Matrix ({method.capitalize()} Method)\n\n"
            plt.figure(figsize=(min(10, len(numeric_cols)*1.2), min(8, len(numeric_cols))))
            sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f", vmin=-1, vmax=1)
            plt.title(f"Correlation Matrix of Selected Features", fontsize=16)
            plt.xticks(rotation=45, ha='right')
            plt.yticks(rotation=0)
            plt.tight_layout()
            img_base64 = _figure_to_base64(plt.gcf())
            result_md += f"![Correlation Heatmap](data:image/png;base64,{img_base64})\n\n"
            result_md += "### Correlation Values:\n" + corr_matrix.to_markdown(floatfmt=".2f")
            
            return result_md

        except Exception as e:
            logger.error(f"Error calculating correlation matrix: {e}", exc_info=True)
            return f"Error calculating correlation matrix: {str(e)}"

    @mcp.tool()
    async def query_table_qa(
        ctx: Context,
        data: Union[List[Dict[str, Any]], pd.DataFrame],
        query: str
    ) -> str:
        """
        Answers natural language questions about tabular data using an LLM.
        Provides profiled column metadata as context to the LLM.
        (Note: Actual LLM call needs to be implemented via mcp_client or similar)
        """
        try:
            df, metadata_map = _ensure_dataframe_and_profile(data, profiler)
            if df is None or metadata_map is None: return "No data available to query or data is empty."

            # Prepare context for LLM
            column_descriptions = "\n".join([f"- {meta.cleaned_name} ({meta.semantic_type}): {meta.description}" for meta in metadata_map.values()])
            
            # For brevity, send only top N rows of data to LLM
            sample_data_json = df.head(10).to_json(orient="records", indent=2)

            llm_prompt = f"""
You are a data analysis assistant. Given the following table data and column descriptions, please answer the user's question.

Column Descriptions:
{column_descriptions}

Sample Data (up to 10 rows):
{sample_data_json}

User Question: {query}

Provide a concise answer based *only* on the provided data and descriptions. If the data doesn't support an answer, say so.
Answer:
"""
            result_md = f"## Query: \"{query}\"\n\n"
            
            # Placeholder for LLM call
            # In a real scenario, you would call your LLM here:
            # llm_response = await ctx.mcp_client.call_llm(prompt=llm_prompt) 
            # For this example, we'll simulate a response or indicate an LLM is needed.
            
            simulated_llm_response = (
                f"To answer the question \"{query}\", I analyzed the provided table data. "
                f"The table has columns: {', '.join([meta.cleaned_name for meta in metadata_map.values()])}. "
                "Based on this structure and the sample data, [simulated LLM answer would go here]. "
                "A more precise answer would require a live LLM query with the full dataset or more sophisticated local query parsing."
            )
            logger.info(f"Prepared prompt for LLM (QA): {llm_prompt[:500]}...") # Log part of the prompt
            
            # Attempt basic parsing for simple "what is the X of Y" type questions
            # This is a very simplified local QA attempt
            parsed_answer = "Could not determine a direct answer locally. An LLM query would be more effective. " + simulated_llm_response
            
            q_lower = query.lower()
            if "what is the" in q_lower or "what's the" in q_lower:
                match_agg = re.search(r"(average|mean|total|sum|count|min|minimum|max|maximum) of (.+)", q_lower)
                if match_agg:
                    agg_func_str, col_hint = match_agg.groups()
                    target_col = _get_col_by_cleaned_name_or_query(col_hint, metadata_map, target_semantic_types=["Currency", "Float", "Integer", "Percentage"])
                    if target_col and pd.api.types.is_numeric_dtype(df[target_col]):
                        series = pd.to_numeric(df[target_col], errors='coerce').dropna()
                        if not series.empty:
                            val = None
                            if agg_func_str in ["average", "mean"]: val = series.mean()
                            elif agg_func_str in ["total", "sum"]: val = series.sum()
                            elif agg_func_str == "count": val = series.count()
                            elif agg_func_str in ["min", "minimum"]: val = series.min()
                            elif agg_func_str in ["max", "maximum"]: val = series.max()
                            
                            if val is not None:
                                parsed_answer = f"The {agg_func_str} of {metadata_map[target_col].cleaned_name} is {val:.2f}."
            
            result_md += parsed_answer
            return result_md

        except Exception as e:
            logger.error(f"Error in query_table_qa: {e}", exc_info=True)
            return f"Error answering question about table: {str(e)}"

    @mcp.tool()
    async def generate_report_from_data(
        ctx: Context,
        data: Union[List[Dict[str, Any]], pd.DataFrame],
        title: str = "Data Analysis Report",
        query: Optional[str] = None # Optional query to guide which analyses are prioritized
    ) -> str:
        """
        Generates a comprehensive Markdown report summarizing and visualizing the data.
        It leverages other analytics tools like summarize_table, visualize_data, etc.
        """
        try:
            df, metadata_map = _ensure_dataframe_and_profile(data, profiler)
            if df is None or metadata_map is None: return "No data to generate report from or data is empty."

            report_parts = [f"# {title}\n_Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}_"]

            # 1. Basic Summary
            summary = await summarize_table(ctx, df.to_dict(orient='records')) # Pass list of dicts
            report_parts.append("\n## 1. Data Overview\n" + summary.replace("## Data Summary by Column", ""))

            # 2. Key Visualizations (try to pick relevant ones)
            report_parts.append("\n## 2. Key Visualizations")
            num_cols = _get_cols_by_semantic_types(metadata_map, ["Currency", "Float", "Integer", "Percentage"])
            cat_cols = _get_cols_by_semantic_types(metadata_map, ["Categorical", "Identifier", "Boolean", "Year"])
            
            if cat_cols and num_cols:
                # Bar chart of a numeric column by a categorical column
                vis_bar = await visualize_data(ctx, df.to_dict(orient='records'), chart_type="bar", x_column_original=cat_cols[0], y_column_original=num_cols[0], query=query)
                report_parts.append(vis_bar.replace("## Data Visualization","### Bar Chart Example"))

            time_col = _get_col_by_semantic_type(metadata_map, "Date") or _get_col_by_semantic_type(metadata_map, "Timestamp")
            if time_col and num_cols:
                 # Line chart of a numeric column over time
                vis_line = await visualize_data(ctx, df.to_dict(orient='records'), chart_type="line", x_column_original=time_col, y_column_original=num_cols[0], query=query)
                report_parts.append(vis_line.replace("## Data Visualization","### Time Series Example"))

            # 3. Trend Analysis (if applicable)
            if time_col and num_cols:
                trend_analysis = await analyze_trend(ctx, df.to_dict(orient='records'), time_column_original=time_col, value_column_original=num_cols[0], query=query)
                report_parts.append("\n## 3. Trend Analysis\n" + trend_analysis.replace(f"## Trend Analysis: {metadata_map[num_cols[0]].cleaned_name} over {metadata_map[time_col].cleaned_name}", ""))
            
            # 4. Anomaly Detection
            if num_cols:
                anomaly_report = await detect_anomalies(ctx, df.to_dict(orient='records'), value_column_original=num_cols[0], time_column_original=time_col, query=query, include_visualization=False) # Keep report concise
                report_parts.append("\n## 4. Anomaly Detection Summary\n" + anomaly_report.replace(f"## Anomaly Detection for {metadata_map[num_cols[0]].cleaned_name}",""))

            # 5. Correlation (if multiple numeric columns)
            if len(num_cols) >= 2:
                corr_report = await correlation_matrix(ctx, df.to_dict(orient='records'), columns_original=num_cols[:5], query=query) # Limit to 5 for report
                report_parts.append("\n## 5. Correlation Insights\n" + corr_report.replace("## Correlation Analysis (Pearson Method)",""))

            # (Optional) Call generate_insights LLM tool for a final summary
            # insights = await generate_insights(ctx, "\n".join(report_parts), query="Summarize this data report")
            # report_parts.append("\n## 6. AI Generated Insights\n" + insights)
            
            return "\n\n".join(report_parts)

        except Exception as e:
            logger.error(f"Error generating report: {e}", exc_info=True)
            return f"Error generating report: {str(e)}"

    @mcp.tool()
    async def export_visual(
        ctx: Context,
        data: Union[List[Dict[str, Any]], pd.DataFrame],
        export_type: str, # "chart" or "table"
        format: str, # "png", "pdf", "csv", "xlsx", "json"
        chart_type: Optional[str] = "bar", # For chart export
        x_column_original: Optional[str] = None, # For chart export
        y_column_original: Optional[str] = None, # For chart export
        query: Optional[str] = None
    ) -> str:
        """
        Exports a chart or table to a specified file format (PNG, PDF, CSV, XLSX, JSON).
        Returns the path to the saved file.
        """
        try:
            df, metadata_map = _ensure_dataframe_and_profile(data, profiler)
            if df is None or metadata_map is None: return "No data to export or data is empty."

            if export_type.lower() == "chart":
                if format.lower() not in ["png", "pdf", "svg"]:
                    return f"Unsupported chart export format: {format}. Use png, pdf, or svg."
                
                # Re-use visualize_data logic but capture figure for saving
                # This is a simplified version. A more robust way would be to refactor visualize_data
                # to return the figure object or have a dedicated plotting function.
                x_col = _get_col_by_cleaned_name_or_query(query, metadata_map, target_semantic_types=["Categorical", "Date", "Timestamp", "Year"], fallback_original_name=x_column_original) or df.columns[0]
                y_col = _get_col_by_cleaned_name_or_query(query, metadata_map, target_semantic_types=["Currency", "Float", "Integer", "Percentage"], fallback_original_name=y_column_original) or df.columns[1]
                
                plt.figure(figsize=(12,7))
                # Basic plot for export - extend with more chart types if needed
                if chart_type.lower() == "bar": sns.barplot(x=x_col, y=y_col, data=df)
                elif chart_type.lower() == "line": sns.lineplot(x=x_col, y=y_col, data=df)
                else: return f"Chart type '{chart_type}' not supported for direct export via this simplified path."
                
                plt.title(f"{metadata_map[y_col].cleaned_name} by {metadata_map[x_col].cleaned_name}", fontsize=16)
                plt.xlabel(metadata_map[x_col].cleaned_name, fontsize=14)
                plt.ylabel(metadata_map[y_col].cleaned_name, fontsize=14)
                plt.xticks(rotation=45, ha='right')
                plt.tight_layout()
                
                file_path = _save_figure_to_file(plt.gcf(), format=format.lower())
                return f"Chart exported successfully to: {file_path}"

            elif export_type.lower() == "table":
                if format.lower() not in ["csv", "xlsx", "json"]:
                    return f"Unsupported table export format: {format}. Use csv, xlsx, or json."
                file_path = _dataframe_to_file(df, metadata_map, format=format.lower())
                return f"Table exported successfully to: {file_path}"
            else:
                return f"Invalid export_type: {export_type}. Must be 'chart' or 'table'."

        except Exception as e:
            logger.error(f"Error exporting visual: {e}", exc_info=True)
            return f"Error exporting visual: {str(e)}"
    
    @mcp.tool()
    async def forecast_revenue(
        ctx: Context,
        data: Union[List[Dict[str, Any]], pd.DataFrame],
        time_column_original: Optional[str] = None,
        value_column_original: Optional[str] = None,
        periods_to_forecast: int = 12, # e.g., 12 months
        query: Optional[str] = None
    ) -> str:
        """
        Forecasts future values for a time series using ARIMA model.
        Uses profiled metadata to identify time and value columns.
        """
        try:
            df, metadata_map = _ensure_dataframe_and_profile(data, profiler)
            if df is None or metadata_map is None: return "No data for forecasting or data is empty."

            time_col = _get_col_by_cleaned_name_or_query(query, metadata_map, target_semantic_types=["Date", "Timestamp", "Year"], fallback_original_name=time_column_original)
            if not time_col: return "Could not identify a time column for forecasting."
            
            value_col = _get_col_by_cleaned_name_or_query(query, metadata_map, target_semantic_types=["Currency", "Float", "Integer"], fallback_original_name=value_column_original)
            if not value_col: return "Could not identify a numeric value column for forecasting."

            df[time_col] = pd.to_datetime(df[time_col], errors='coerce')
            df[value_col] = pd.to_numeric(df[value_col], errors='coerce')
            df_forecast = df.dropna(subset=[time_col, value_col]).set_index(time_col)[value_col]

            if len(df_forecast) < 20: # ARIMA needs sufficient data
                return f"Not enough data points ({len(df_forecast)}) in '{metadata_map[value_col].cleaned_name}' for reliable forecasting. At least 20 are recommended."

            # Fit ARIMA model (auto-ARIMA can be slow, using simple order for example)
            # For robust forecasting, consider pmdarima.auto_arima or manual order selection
            try:
                model = ARIMA(df_forecast, order=(5,1,0)) # Example order
                model_fit = model.fit()
                forecast_result = model_fit.forecast(steps=periods_to_forecast)
            except Exception as arima_e:
                logger.warning(f"ARIMA(5,1,0) failed: {arima_e}. Trying SARIMAX.")
                # Fallback to simpler SARIMAX if ARIMA fails (e.g. non-stationarity issues)
                model = SARIMAX(df_forecast, order=(1,1,1), seasonal_order=(0,0,0,0))
                model_fit = model.fit(disp=False)
                forecast_result = model_fit.forecast(steps=periods_to_forecast)


            forecast_dates = pd.date_range(start=df_forecast.index[-1], periods=periods_to_forecast + 1, freq=pd.infer_freq(df_forecast.index))[1:]
            forecast_series = pd.Series(forecast_result, index=forecast_dates)

            value_label = metadata_map[value_col].cleaned_name
            time_label = metadata_map[time_col].cleaned_name
            result_md = f"## {value_label} Forecast ({periods_to_forecast} Periods)\n\n"
            
            plt.figure(figsize=(14,7))
            plt.plot(df_forecast.index, df_forecast, label='Historical Data')
            plt.plot(forecast_series.index, forecast_series, label='Forecast', color='red')
            plt.title(f"Forecast for {value_label}", fontsize=16)
            plt.xlabel(time_label, fontsize=14)
            plt.ylabel(value_label, fontsize=14)
            plt.legend()
            plt.tight_layout()
            img_base64 = _figure_to_base64(plt.gcf())
            result_md += f"![Forecast Plot](data:image/png;base64,{img_base64})\n\n"
            
            forecast_df = forecast_series.reset_index()
            forecast_df.columns = [time_label, f"Forecasted {value_label}"]
            result_md += "### Forecasted Values:\n" + forecast_df.to_markdown(index=False, floatfmt=".2f")
            
            return result_md

        except Exception as e:
            logger.error(f"Error forecasting revenue: {e}", exc_info=True)
            return f"Error forecasting data: {str(e)}"

    @mcp.tool()
    async def generate_insights(
        ctx: Context,
        data_summary_or_report: str, # Markdown from other tools
        query: Optional[str] = "What are the key insights from this data?"
    ) -> str:
        """
        Uses an LLM to generate textual insights, explanations, or summaries from processed data or reports.
        (Note: Actual LLM call needs mcp_client to be available in ctx)
        """
        try:
            if not hasattr(ctx, 'mcp_client') or not hasattr(ctx.mcp_client, 'call_llm'):
                return "LLM client not available in context. Cannot generate insights."

            llm_prompt = f"""
Analyze the following data summary/report and provide key insights based on the user's query.

User Query: {query}

Data/Report:
---
{data_summary_or_report}
---

Key Insights (be concise and focus on actionable information):
"""
            # Simulated LLM call
            # llm_response_data = await ctx.mcp_client.call_llm(prompt=llm_prompt) # or construct messages list
            # insights = llm_response_data.get("choices")[0].get("message").get("content")
            
            # Placeholder for now as direct LLM call mechanism is not fully defined here
            insights = (
                f"Based on the query \"{query}\" and the provided data/report:\n"
                "- Insight 1: [Simulated insight based on patterns in the data/report]\n"
                "- Insight 2: [Another simulated observation]\n"
                "- Recommendation: [A possible action based on the insights]\n"
                "(This is a placeholder. A live LLM would provide more specific insights.)"
            )
            logger.info(f"Generated insights for query: {query}")
            return f"## AI Generated Insights\n\n{insights}"

        except Exception as e:
            logger.error(f"Error generating insights: {e}", exc_info=True)
            return f"Error generating insights: {str(e)}"

    @mcp.tool()
    async def rank_clients(
        ctx: Context,
        data: Union[List[Dict[str, Any]], pd.DataFrame],
        rank_by_original: List[str], # Columns to rank by (original names)
        ascending: Union[List[bool], bool] = False, # Corresponding sort orders
        id_column_original: Optional[str] = None,
        query: Optional[str] = None,
        top_n: Optional[int] = None
    ) -> str:
        """
        Ranks clients or entities based on multiple criteria.
        Uses profiled metadata to identify ID column if not specified.
        """
        try:
            df, metadata_map = _ensure_dataframe_and_profile(data, profiler)
            if df is None or metadata_map is None: return "No data for ranking or data is empty."

            valid_rank_cols = [col for col in rank_by_original if col in metadata_map and metadata_map[col].semantic_type in ["Currency", "Float", "Integer", "Percentage", "Year"]]
            if not valid_rank_cols: return "No valid numeric columns provided for ranking."

            df_rank = df.copy()
            for col in valid_rank_cols: # Ensure numeric
                 df_rank[col] = pd.to_numeric(df_rank[col], errors='coerce')
            
            # Handle ascending: if single bool, apply to all; if list, match with rank_by_original
            if isinstance(ascending, bool):
                asc_list = [ascending] * len(valid_rank_cols)
            elif isinstance(ascending, list) and len(ascending) == len(valid_rank_cols):
                asc_list = ascending
            else:
                asc_list = [False] * len(valid_rank_cols) # Default to descending

            df_ranked = df_rank.sort_values(by=valid_rank_cols, ascending=asc_list).dropna(subset=valid_rank_cols)
            
            if top_n and top_n > 0:
                df_ranked = df_ranked.head(top_n)

            id_col = _get_col_by_cleaned_name_or_query(query, metadata_map, target_semantic_types=["Identifier", "Categorical"], fallback_original_name=id_column_original)
            if not id_col: id_col = _get_col_by_semantic_type(metadata_map, "Identifier") or df.columns[0]
            
            display_cols = [id_col] + valid_rank_cols
            # Filter display_cols to those actually in df_ranked
            display_cols = [col for col in display_cols if col in df_ranked.columns]


            result_md = f"## Ranked Clients/Entities\n\n"
            result_md += "Ranked by: " + ", ".join([metadata_map[col].cleaned_name for col in valid_rank_cols]) + "\n\n"
            
            # Create a new DataFrame for display with cleaned names
            df_display = df_ranked[display_cols].copy()
            df_display.columns = [metadata_map.get(col, ColumnMetadata(original_name=col, cleaned_name=col, semantic_type="Unknown", pandas_dtype="object", description="", count=0,null_count=0,null_ratio=0,unique_count=0,unique_ratio=0,profile_summary="")).cleaned_name for col in display_cols]
            
            result_md += df_display.to_markdown(index=False, floatfmt=".2f")
            return result_md

        except Exception as e:
            logger.error(f"Error ranking clients: {e}", exc_info=True)
            return f"Error ranking data: {str(e)}"

    @mcp.tool()
    async def filter_table(
        ctx: Context,
        data: Union[List[Dict[str, Any]], pd.DataFrame],
        filter_query: str, # Natural language filter query, e.g., "revenue > 1M and region is USA"
        output_format: str = "markdown" # markdown, json
    ) -> str:
        """
        Filters tabular data based on a natural language query.
        Uses profiled metadata to understand column types for filtering.
        (Note: Full NLP parsing of filter_query is complex; this uses a simplified approach.)
        """
        try:
            df, metadata_map = _ensure_dataframe_and_profile(data, profiler)
            if df is None or metadata_map is None: return "No data to filter or data is empty."

            df_filtered = df.copy()
            conditions_applied = []

            # Simplified parsing: "column_name operator value" and "column_name is value"
            # And "column_name contains value" for strings
            # Operators: >, <, >=, <=, ==, !=, is, contains
            # Multiple conditions can be chained with "and" (for now, only "and" is implicitly handled by sequential filtering)
            
            # Split query by "and" to handle multiple conditions sequentially
            sub_queries = re.split(r'\s+and\s+', filter_query, flags=re.IGNORECASE)

            for sub_q in sub_queries:
                sub_q = sub_q.strip()
                # Pattern: col_name operator value (e.g. "revenue > 1000000")
                match_op_val = re.match(r"(.+?)\s*([><=!]+|is(?:\s+not)?|contains(?:\s+not)?)\s*(.+)", sub_q, re.IGNORECASE)
                
                if match_op_val:
                    col_hint, operator, value_str = match_op_val.groups()
                    col_hint = col_hint.strip()
                    operator = operator.strip().lower()
                    value_str = value_str.strip().strip("'\"") # Remove quotes

                    target_col = _get_col_by_cleaned_name_or_query(col_hint, metadata_map)
                    if not target_col: 
                        logger.warning(f"Filter: Could not find column for hint '{col_hint}' in query '{sub_q}'")
                        continue
                    
                    meta = metadata_map[target_col]
                    series = df_filtered[target_col]
                    condition_applied_text = f"'{meta.cleaned_name}' {operator} '{value_str}'"

                    try:
                        if meta.semantic_type in ["Currency", "Float", "Integer", "Percentage", "Year"]:
                            series_numeric = pd.to_numeric(series, errors='coerce')
                            value_numeric = float(value_str.replace(',',''))
                            if operator == '>': df_filtered = df_filtered[series_numeric > value_numeric]
                            elif operator == '<': df_filtered = df_filtered[series_numeric < value_numeric]
                            elif operator == '>=': df_filtered = df_filtered[series_numeric >= value_numeric]
                            elif operator == '<=': df_filtered = df_filtered[series_numeric <= value_numeric]
                            elif operator == '==' or operator == 'is': df_filtered = df_filtered[series_numeric == value_numeric]
                            elif operator == '!=': df_filtered = df_filtered[series_numeric != value_numeric]
                            else: continue # Unknown operator for numeric
                        elif meta.semantic_type in ["Date", "Timestamp"]:
                            series_dt = pd.to_datetime(series, errors='coerce')
                            value_dt = pd.to_datetime(value_str)
                            if operator == '>': df_filtered = df_filtered[series_dt > value_dt]
                            elif operator == '<': df_filtered = df_filtered[series_dt < value_dt]
                            # ... add more date operators
                        else: # Categorical, String, Identifier, Boolean
                            series_str = series.astype(str).str.lower()
                            value_lower = value_str.lower()
                            if operator == 'is' or operator == '==': df_filtered = df_filtered[series_str == value_lower]
                            elif operator == 'is not' or operator == '!=': df_filtered = df_filtered[series_str != value_lower]
                            elif operator == 'contains': df_filtered = df_filtered[series_str.str.contains(value_lower, na=False)]
                            elif operator == 'contains not': df_filtered = df_filtered[~series_str.str.contains(value_lower, na=False)]
                            else: continue # Unknown operator for string
                        conditions_applied.append(condition_applied_text)
                    except Exception as filter_ex:
                        logger.warning(f"Could not apply filter '{sub_q}' on column '{target_col}': {filter_ex}")
                        continue
            
            result_md = f"## Filtered Table\n\n"
            if conditions_applied:
                 result_md += "Applied filters: " + ", and ".join(conditions_applied) + "\n"
            else:
                 result_md += "No valid filters could be applied from the query.\n"
            result_md += f"Found {len(df_filtered)} matching records.\n\n"

            if output_format.lower() == "json":
                return df_filtered.to_json(orient="records", indent=2)
            
            # For markdown, use the main formatter tool for consistency
            # This requires data to be list of dicts
            # Re-profile the filtered data for accurate final formatting
            filtered_data_list = df_filtered.to_dict(orient='records')
            if not filtered_data_list: return result_md + "No records match the filter criteria."

            # Call format_table_for_llm from app.tools.data_processor
            # This creates a dependency, or we can duplicate some formatting logic here.
            # For now, let's assume a simpler markdown for filtered results.
            # A better way would be to call the format_table_for_llm tool if it's registered.
            
            # Simplified markdown output:
            df_display_filtered = df_filtered.copy()
            df_display_filtered.columns = [metadata_map.get(col, ColumnMetadata(original_name=col, cleaned_name=col, semantic_type="Unknown", pandas_dtype="object", description="", count=0,null_count=0,null_ratio=0,unique_count=0,unique_ratio=0,profile_summary="")).cleaned_name for col in df_filtered.columns]
            result_md += df_display_filtered.head(20).to_markdown(index=False, floatfmt=".2f") # Show top 20
            if len(df_filtered) > 20:
                result_md += f"\n\n(...and {len(df_filtered) - 20} more rows)"

            return result_md

        except Exception as e:
            logger.error(f"Error filtering table: {e}", exc_info=True)
            return f"Error filtering table: {str(e)}"

    logger.info("Registered analytics suite tools.")

